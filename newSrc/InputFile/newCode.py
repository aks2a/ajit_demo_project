# -*- coding: utf-8 -*-
"""project3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uiAEzPmsnux6urpQzIpRWDCdnzkaAZfy
"""

!pip3 install pyspark



from pyspark.sql.functions import *

# spark Session 
from pyspark.sql import SparkSession
spark = SparkSession.builder.master('local[1]').appName("basic Program").getOrCreate()

rawFile_df = spark.read.text("/content/drive/MyDrive/input_volume_request.txt")

# create row data csv file 
rawFile_df.write.format("csv").mode('overwrite').save("/content/rawData.csv")

rawFile_df.show(10,truncate=False)

#print()
#rawFile_df.printSchema()
#type(rawFile_df)
# print(rawFile_df.count())
#print(rawFile_df.column())

host_p = r'([\S+\.]+)'
time_pattern = r'(\d+/\w+/\d+[:\d]+)'
GET = r'GET|POST|HEAD'
request_p = r'\s\S+\sHTTP/1.1"'
status_p = r'\s\d{3}\s'
size_p = r'\s(\d+)\s"'
ree = r'("https(\S+)")'
usery = r'"(Mozilla|Dalvik|Goog|torob|Bar).\d\S+\s\((\S\w+;?\s\S+(\s\d\.(\d\.)?\d)?)'
spe_char = r'[%,|"-&.=?-]'

# *********** ARRANGING THE EXTRACTING DATA INTO DATAFRAME ***************************
rawFile_df = rawFile_df.withColumn("id", monotonically_increasing_id()) \
    .select('id', regexp_extract('value', host_p, 1).alias('clientip')
            , regexp_extract('value', time_pattern, 1).alias('datetime_confirmed')
            , regexp_extract('value', GET, 0).alias("method_GET")
            , regexp_extract('value', request_p, 0).alias('request')
            , regexp_extract('value', status_p, 0).alias('status_code')
            , regexp_extract('value', size_p, 1).alias('size')
            , regexp_extract('value', ree, 1).alias('referer')
            , regexp_extract('value', usery, 0).alias('user_agent'))
rawFile_df.show(truncate=False)



#delete any special characters in the request column(% ,- ? =)
rawFile_df = rawFile_df.withColumn('request', regexp_replace('request', r'[%,|"-&.=?-]', ''))

#Size to KB
rawFile_df = rawFile_df.withColumn('size', round(rawFile_df.size / 1024))
rawFile_df.show(truncate=False)

#remove_empty_null
rawFile_df = rawFile_df.select(
            [when(col(c) == "-", None).otherwise(col(c)).alias(c) for c in rawFile_df.columns])
rawFile_df.show(truncate=False)

#count_null_row_wise
rawFile_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in rawFile_df.columns])
rawFile_df.show(truncate=False)



# Raw Layer Save File 
rawFile_df.write.mode('overwrite').csv("/content/drive/MyDrive/output_files/raw_layer/", header = 'True')

#Cleansed Layer

cleanFile_df = spark.read.csv(
        "/content/drive/MyDrive/output_files/raw_layer/part-00000-a7ffe5c2-c07b-4c22-b4bc-ae6c7d1894b2-c000.csv",
        header=True)

#print(cleanFile_df.count())

#datetime_formats
import sys
import time

cleanFile_df = cleanFile_df.withColumn("datetime_confirmed", split(cleanFile_df["datetime_confirmed"], ' ').getItem(0)).withColumn("datetime_confirmed",to_timestamp("datetime_confirmed",'dd/MMM/yyyy:HH:mm:ss'))\
.withColumn("datetime_confirmed",to_timestamp("datetime_confirmed",'MMM/dd/yyyy:HH:mm:ss'))
cleanFile_df.show(truncate=False)

cleanFile_df=cleanFile_df.na.fill("NA")
#referer_present
cleanFile_df = cleanFile_df.withColumn("referer_present(Y/N)",
                                      when(col("referer") == "NA", "N") \
                                      .otherwise("Y"))

cleanFile_df.show(20,truncate=False)

# remove_referer
cleanFile_df = cleanFile_df.drop("referer")
cleanFile_df.show(truncate=False)

# save file Cleansed layer
cleanFile_df.write.csv("/content/drive/MyDrive/output_files/cleansed_layer/", header = 'True',mode='overwrite')
cleanFile_df.show(truncate=False)
# save File Curated log File
#cleanFile_df.write.format("csv").mode('overwrite').save("/content/curatedLogFile.csv", header='True')
#cleanFile_df.write.mode('overwrite').csv("/content/Output/curateddLogDetails.csv", header = 'True')
#cleanFile_df.write.csv("/content/output/curatedLogFile.csv", mode='overwrite', header = 'True')
#cleanFile_df.write.mode('overwrite').csv("/content/Output/curatedLogFile.csv", header = 'True')



# curated

# Curated Data
curatedFile_df = spark.read.csv("/content/drive/MyDrive/output_files/cleansed_layer/part-00000-656d0071-0164-4e90-8f30-57cafdffbdf5-c000.csv",header=True)
curatedFile_df.show(truncate=False)

#print(curatedFile_df.count())

curatedFile_df.write.csv("/content/drive/MyDrive/output_files/curated_layer/", header = 'True',mode='overwrite')

# add column hour,Get,Post,Head
curatedFile_df = curatedFile_df.withColumn("No_get", when(col("method_GET") == "GET", "GET")) \
            .withColumn("No_post", when(col("method_GET") == "POST", "POST")) \
            .withColumn("No_Head", when(col("method_GET") == "HEAD", "HEAD")) \
            .withColumn("day", to_date(col("datetime_confirmed"))) \
            .withColumn("hour", hour(col("datetime_confirmed"))) \
            .withColumn("day_hour", concat(col("day"), lit(" "), col("hour")))

#curatedFile_df.show()

# perform aggregation per device

curatedFile_df = curatedFile_df.select("id", "day_hour", "clientIp", "no_get", "no_post", "no_head") \
            .groupBy("day_hour", "clientip") \
            .agg(count("id").alias("id"),
                 count(col("No_get")).alias("no_get"),
                 count(col("No_post")).alias("no_post"),
                 count(col("No_head")).alias("no_head")) \
            # .orderBy(col("row_id").desc())

curatedFile_df.show()

#print(curatedFile_df.count())

# write_to_perDevice
curatedFile_df.write.mode('overwrite').csv("/content/drive/MyDrive/output_files/Aggregation/per_device/", header = 'True')

# perform aggregation across device

curatedFile_df = curatedFile_df \
            .groupBy("day_hour") \
            .agg(
                count("clientip").alias("no_of_clients"),
                count("id").alias("id"),
                count(col("No_get")).alias("no_get"),
                count(col("No_post")).alias("no_post"),
                count(col("No_head")).alias("no_head")
                )
curatedFile_df.show()

# write_to_AcrossPerDevice
curatedFile_df.write.mode('overwrite').csv("/content/drive/MyDrive/output_files/Aggregation/across_device/", header = 'True')

#curatedFile_df.repartition(1)

# write to hive
#curatedFile_df.write.saveAsTable('logPerDevice')



# snowflake
"""
def loadData_snowflake(self):
        SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
        snowflake_database = "aks_DB"
        snowflake_schema = "PUBLIC"
        source_table_name = "AGENTS"
        snowflake_options = {
            "sfUrl": "******.snowflakecomputing.com",
            "sfUser": "********",
            "sfPassword": "********",
            "sfDatabase": "aks_DB",
            "sfSchema": "PUBLIC",
            "sfWarehouse": "COMPUTE_WH"
        }
        self.curatedFile_df = self.curatedFile_df.write.parquet(
            "s3://mkc-tutorial-destination-bucket-ajit/tutorial/extractData/curatedLogFile.parquet", mode="overwrite")

        
        self.curatedFile_df = self.curatedFile_df.select("id", "clientip", "datetime_confirmed", "method_GET", "status_code", "size", "user_agent",
                        "referer_present")
        self.curatedFile_df.write.format("snowflake") \
            .options(**snowflake_options) \
            .option("dbtable", "curated").mode("overwrite") \
            .save()
"""

from os import path
from pathlib import Path

#l=Path('C:\\Users\\ajit.ks\\Desktop\\training_dataProject.txt')
new_l=Path('/content/increaseVolume.txt')
l = Path('/content/training_dataProject.txt')
#new_l=write.mode('overwrite').csv("/content/Output/increaseVolume.txt")

p=open(l)
count = 0
increment = 1
increm = 1
new_data=open(new_l,'a')

def incremental():

    p.seek(0)
    for line_num, i in enumerate(p):
        global increment
        global count
        global increm
        l_digit = int(i.strip(" ").split(" ")[0].split(".")[-1])
        time_hour = int(i.strip(" ").split(" ")[3].split(":")[1])
        if (line_num + 1) % 10 == 0:
            incr = int(l_digit + increment)
            i = i.replace(str(l_digit), str(incr), 1)
            incr_hour = int(time_hour + increm)
            i = i.replace(":0"+str(time_hour), ":0"+str(incr_hour))
        if (line_num + 1) % 9 == 0:
            i = i.replace('GET', 'POST', 1)
        new_data.write(i)
        print(count)
        count = count + 1
        increment = +1
        increm = 1

num_record = int(input("Enter the number of records wanted(in Lakhs): "))
num_acc = int((num_record * 1000))

for i in range(num_acc):
    print(str(i+1))
    incremental()

# Author: K.Nayan
from time import sleep
from json import dumps
# from kafka import KafkaProducer

# read the file
import time


class augment:
    produce = KafkaProducer(bootstrap_servers=['b-2.s3sinkcluster.uj4gy1.c2.kafka.ap-south-1.amazonaws.com:9092','b-1.s3sinkcluster.uj4gy1.c2.kafka.ap-south-1.amazonaws.com:9092'],
                            value_serializer=lambda x: dumps(x).encode('utf-8'))
    topic_name = 'demoproject'
    input_filename = "/home/ec2-user/log_data_ip_request.txt"
    #input_filename = "log_data_ip_request.txt"
    count = 0
    incrementBy = 0
    input_file_object = open(input_filename, 'r')

    file_object = open('IncreasedInput.txt', 'a')

    def changer(self):
        self.input_file_object.seek(0)
        for line_num, line in enumerate(self.input_file_object):
            last_digit = int(line.strip(" ").split(" ")[0].split(".")[-1])
            hour_digit = int(line.split(" ")[3].split(":")[1])
            day_digit = int(line.split(" ")[3].split("/")[0][1:])
            if (line_num) < 10:
                # change the
                temp_hour_changer = hour_digit + self.incrementBy
                temp_digit_changer = last_digit + self.incrementBy
                temp_day_changer = day_digit + self.incrementBy
                # temp_digit_changer = last_digit + 1

                if temp_day_changer > 31: temp_day_changer = 31
                if temp_hour_changer > 23: temp_hour_changer = 23

                line = line.replace(str('.' + str(last_digit)), str('.' + str(temp_digit_changer)), 1)
                line = line.replace(str(str(hour_digit) + ':'), str(str(temp_hour_changer) + ':'), 1)
                line = line.replace(str('[' + str(day_digit) + '/'), str('[' + str(temp_day_changer) + '/'), 1)

                # if (line_num + 1) % 9 == 0:
                if (self.incrementBy >= 1):
                    line = line.replace("GET", "POST", 1)

            self.count += 1
            print("number of records inserted:--> " + str(self.count))
            self.produce.send(self.topic_name, value=line)
            # self.file_object.write(line)
        # if self.incrementBy < 1:
        self.incrementBy += 1

    def closer(self):
        self.input_file_object.close()
        self.file_object.close()
        print("file closed")


if __name__ == "__main__":
    # Number of records wanted in Lakhs

    Num_of_records_wanted_in_lakhs = int(input("Enter the number of records wanted(in Lakhs): "))
    # Number of times inp file to be accessed
    num_access = int((Num_of_records_wanted_in_lakhs * 100000) / 319)
    print(num_access)

    augment = augment()
    for i in range(num_access):
        print("reading for " + str(i + 1) + " time")
        # time.sleep(1)
        augment.changer()

    augment.closer()