# -*- coding: utf-8 -*-
"""project4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fnJh4pmxMRVIkFJ9XlmBNWa_hb1cB2jT
"""

!pip3 install pyspark

from pyspark.sql.functions import *

# spark Session 
from pyspark.sql import SparkSession
spark = SparkSession.builder.master('local[1]').appName("basic Program").getOrCreate()

rawFile_df = spark.read.text("/content/drive/MyDrive/new_log.txt")

# create row data csv file 
#rawFile_df.write.format("csv").mode('overwrite').save("/content/drive/MyDrive/output_files/Raw_csv")

rawFile_df.show(10,truncate=False)
rawFile_df.count()

host_p = r'([\S+\.]+)'
time_pattern = r'(\d+/\w+/\d+[:\d]+)'
GET = r'GET|POST|HEAD'
request_p = r'\s\S+\sHTTP/1.1"'
status_p = r'\s\d{3}\s'
size_p = r'\s(\d+)\s"'
ree = r'("https(\S+)")'
usery = r'"(Mozilla|Dalvik|Goog|torob|Bar).\d\S+\s\((\S\w+;?\s\S+(\s\d\.(\d\.)?\d)?)'
spe_char = r'[%,|"-&.=?-]'

# *********** ARRANGING THE EXTRACTING DATA INTO DATAFRAME ***************************
rawFile_df = rawFile_df.select( regexp_extract('value', host_p, 1).alias('clientip')
            , regexp_extract('value', time_pattern, 1).alias('datetime_confirmed')
            , regexp_extract('value', GET, 0).alias("method_GET")
            , regexp_extract('value', request_p, 0).alias('request')
            , regexp_extract('value', status_p, 0).alias('status_code')
            , regexp_extract('value', size_p, 1).alias('size')
            , regexp_extract('value', ree, 1).alias('referer')
            , regexp_extract('value', usery, 0).alias('user_agent'))
# rawFile_df = rawFile_df.drop_duplicates(["clientip","datetime_confirmed","method_GET"])\
        
rawFile_df.show(truncate=False)

# drop duplicate Value 
rawFile_df = rawFile_df.drop_duplicates(["clientip","datetime_confirmed","method_GET"])\
.drop("id")
rawFile_df = rawFile_df.withColumn("id", monotonically_increasing_id())
rawFile_df = rawFile_df.select("id","clientip","datetime_confirmed","method_GET","request","status_code","size","referer","user_agent")
rawFile_df.show(truncate=False)
rawFile_df.count()

#delete any special characters in the request column(% ,- ? =)
rawFile_df = rawFile_df.withColumn('request', regexp_replace('request', r'[%,|"-&.=?-]', ''))

#Size to KB
rawFile_df = rawFile_df.withColumn('size', round(rawFile_df.size / 1024))
rawFile_df.show(truncate=False)

#remove_empty_null
rawFile_df = rawFile_df.select(
            [when(col(c) == "", "NA").otherwise(col(c)).alias(c) for c in rawFile_df.columns])
rawFile_df.show(truncate=False)

#count_null_row_wise
#rawFile_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in rawFile_df.columns])
#rawFile_df.show(truncate=False)

# Raw Layer Save File 
rawFile_df.write.mode('overwrite').csv("/content/drive/MyDrive/output_files/raw_layer/", header = 'True')

#Cleansed Layer

cleanFile_df = spark.read.csv(
        "/content/drive/MyDrive/output_files/raw_layer/part-00000-eacad4df-971d-47d6-9738-1e94bb436db9-c000.csv", header = 'True')

#print(cleanFile_df.count())

#datetime_formats
import sys
import time

cleanFile_df = cleanFile_df.withColumn("datetime_confirmed", split(cleanFile_df["datetime_confirmed"], ' ').getItem(0)).withColumn("datetime_confirmed",to_timestamp("datetime_confirmed",'dd/MMM/yyyy:HH:mm:ss'))\
.withColumn("datetime_confirmed",to_timestamp("datetime_confirmed",'MMM/dd/yyyy:HH:mm:ss'))
cleanFile_df.show(truncate=False)

cleanFile_df=cleanFile_df.na.fill("NA")
#referer_present
cleanFile_df = cleanFile_df.withColumn("referer_present(Y/N)",
                                      when(col("referer") == "NA", "N") \
                                      .otherwise("Y"))

cleanFile_df.show(20,truncate=False)

# remove_referer
cleanFile_df = cleanFile_df.drop("referer")
cleanFile_df.show(truncate=False)

# save file Cleansed layer
cleanFile_df.write.csv("/content/drive/MyDrive/output_files/cleansed_layer/", header = 'True',mode='overwrite')
cleanFile_df.show(truncate=False)

# curated

# Curated Data
curatedFile_df = spark.read.csv("/content/drive/MyDrive/output_files/cleansed_layer/part-00000-0f79edda-a8a2-4482-97f0-87f9a4d83c25-c000.csv",header=True)
curatedFile_df.show(truncate=False)

#print(curatedFile_df.count())

curatedFile_df.write.csv("/content/drive/MyDrive/output_files/curated_layer/", header = 'True',mode='overwrite')

# add column hour,Get,Post,Head
curatedFile_df = curatedFile_df.withColumn("No_get", when(col("method_GET") == "GET", "GET")) \
            .withColumn("No_post", when(col("method_GET") == "POST", "POST")) \
            .withColumn("No_Head", when(col("method_GET") == "HEAD", "HEAD")) \
            .withColumn("day", to_date(col("datetime_confirmed"))) \
            .withColumn("hour", hour(col("datetime_confirmed"))) \
            .withColumn("day_hour", concat(col("day"), lit(" "), col("hour")))

#add_column_df.show()

# perform aggregation per device

per_device_df = curatedFile_df.select("id", "day_hour", "clientIp", "no_get", "no_post", "no_head") \
            .groupBy("day_hour", "clientip") \
            .agg(count("id").alias("count_Row"),
                 count(col("No_get")).alias("Get"),
                 count(col("No_post")).alias("Post"),
                 count(col("No_head")).alias("Head")) \
            # .orderBy(col("row_id").desc())

per_device_df.show()
per_device_df.count()

#print(curatedFile_df.count())

# write_to_perDevice
per_device_df.write.mode('overwrite').csv("/content/drive/MyDrive/output_files/Aggregation/per_device/", header = 'True')

# perform aggregation across device

across_device_df = curatedFile_df \
            .groupBy("day_hour") \
            .agg(
                count("clientip").alias("no_of_clients"),
                count("id").alias("count_Row"),
                count(col("No_get")).alias("get"),
                count(col("No_post")).alias("post"),
                count(col("No_head")).alias("head")
                )
across_device_df.show()
across_device_df.count()

# write_to_AcrossPerDevice
across_device_df.write.mode('overwrite').csv("/content/drive/MyDrive/output_files/Aggregation/across_device/", header = 'True')

# snowflake

def write_to_snowflake(self):
      SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
      snowflake_database="ajitdb"
      snowflake_schema="public"
      target_table_name="curated_logs"
      snowflake_options = {
        "sfUrl": "jn94146.ap-south-1.aws.snowflakecomputing.com",
        "sfUser": "sushantsangle",
        "sfPassword": "Stanford@01",
        "sfDatabase": snowflake_database,
        "sfSchema": snowflake_schema,
        "sfWarehouse": "curated_snowflake"
    }
      spark=SparkSession.builder\
        .appName("Demo_Project").enableHiveSupport().getOrCreate()
      df = spark.read\
        .format('csv').load('s3://ajit-db/fileName/',header=True)# file path
      df1 = df.select("*")
      df1.write.format("snowflake")\
        .options(**snowflake_options)\
        .option("dbtable", "rawtable")\
        .option("header","true")\
        .mode("overwrite")\
        .save()